"""
=================================================================
SEMANTIC SEARCH & Q&A SYSTEM - 100% FREE VERSION
=================================================================
Production-ready semantic search and question answering system
Using completely free tools and APIs

Free Tools Used:
â€¢ Sentence Transformers (Free local embeddings)
â€¢ FAISS (Facebook's similarity search)
â€¢ Groq API (Free LLM API)
â€¢ FastAPI (Free web framework)
â€¢ Sample Stack Overflow data

Setup:
1. Run in Google Colab or Jupyter
2. Get free Groq API key: https://console.groq.com
3. Install dependencies and start!
"""

# =================================================================
# SECTION 1: INSTALLATION & IMPORTS
# =================================================================

import sys
import subprocess
import warnings
warnings.filterwarnings('ignore')

def install_packages():
    """Install all required packages"""
    packages = [
        'sentence-transformers',  # Free embeddings
        'faiss-cpu',             # Similarity search
        'fastapi',               # Web framework
        'uvicorn[standard]',     # ASGI server
        'pydantic',              # Data validation
        'numpy',                 # Numerical operations
        'pandas',                # Data handling
        'requests',              # HTTP client
        'scikit-learn',          # ML utilities
        'nest-asyncio',          # Async support for notebooks
    ]
    
    print("ðŸ“¦ Installing required packages...")
    for package in packages:
        try:
            subprocess.check_call(
                [sys.executable, "-m", "pip", "install", "-q", package],
                stdout=subprocess.DEVNULL
            )
            print(f"  âœ“ {package}")
        except:
            print(f"  âœ— {package} (may already be installed)")
    
    print("\nâœ… Installation complete!")

# Run installation
install_packages()

# Import libraries
import numpy as np
import pandas as pd
import requests
import json
import time
import faiss
import pickle
import nest_asyncio
from pathlib import Path
from typing import List, Dict, Optional, Tuple, Any
from dataclasses import dataclass, asdict
from datetime import datetime
from sentence_transformers import SentenceTransformer
from fastapi import FastAPI, HTTPException, Query
from pydantic import BaseModel, Field
import uvicorn
from sklearn.metrics.pairwise import cosine_similarity

print("\nâœ… All imports successful!")

# Enable async in notebooks
nest_asyncio.apply()

# =================================================================
# SECTION 2: CONFIGURATION
# =================================================================

@dataclass
class SystemConfig:
    """System configuration"""
    # API Configuration
    groq_api_key: str = ""
    groq_api_url: str = "https://api.groq.com/openai/v1/chat/completions"
    groq_model: str = "llama-3.1-8b-instant"
    
    # Embedding Configuration
    embedding_model_name: str = "all-MiniLM-L6-v2"  # Fast & free
    embedding_dimension: int = 384
    
    # Search Configuration
    top_k_results: int = 3
    similarity_threshold: float = 0.5
    use_approximate_search: bool = True
    
    # Generation Configuration
    max_tokens: int = 500
    temperature: float = 0.3
    
    # API Configuration
    api_host: str = "127.0.0.1"
    api_port: int = 8000
    
    # Cache Configuration
    enable_cache: bool = True
    cache_ttl_seconds: int = 3600

# =================================================================
# SECTION 3: DATA MODELS
# =================================================================

class SearchRequest(BaseModel):
    """Search request model"""
    query: str = Field(..., description="The search query")
    use_approximate: bool = Field(True, description="Use approximate search")
    top_k: int = Field(1, description="Number of results to return")
    
class SearchResponse(BaseModel):
    """Search response model"""
    success: bool
    query: str
    answer: str
    source_document: Dict[str, Any]
    search_method: str
    latency_ms: float
    
class Document(BaseModel):
    """Document model"""
    id: int
    question: str
    answer: str
    similarity_score: Optional[float] = None

# =================================================================
# SECTION 4: SAMPLE DATA GENERATOR
# =================================================================

class DataGenerator:
    """Generate sample Stack Overflow Q&A data"""
    
    @staticmethod
    def generate_sample_data(num_samples: int = 100) -> pd.DataFrame:
        """Generate sample Q&A pairs"""
        
        # Sample programming questions and answers
        qa_pairs = [
            # Python/Pandas
            ("How do I concatenate two pandas DataFrames?", 
             "Use pd.concat([df1, df2]) to concatenate DataFrames vertically, or use df1.append(df2). For horizontal concatenation, use pd.concat([df1, df2], axis=1)."),
            
            ("How to remove duplicates from pandas DataFrame?",
             "Use df.drop_duplicates() to remove duplicate rows. You can specify subset=['col1', 'col2'] to consider only specific columns."),
            
            ("How to handle missing values in pandas?",
             "Use df.fillna(value) to fill missing values, df.dropna() to drop rows with missing values, or df.interpolate() for numerical data."),
            
            ("How to merge two DataFrames in pandas?",
             "Use pd.merge(df1, df2, on='key') for SQL-style joins. You can specify how='left', 'right', 'outer', or 'inner' for different join types."),
            
            ("How to group data in pandas?",
             "Use df.groupby('column').agg({'col': 'mean'}) to group data and apply aggregation functions like sum, mean, count, etc."),
            
            # Python General
            ("How do I read a file in Python?",
             "Use 'with open(filename, 'r') as f: content = f.read()' to safely read file contents. This automatically closes the file."),
            
            ("How to create a list comprehension in Python?",
             "Use [expression for item in iterable if condition]. Example: [x**2 for x in range(10) if x % 2 == 0] creates squares of even numbers."),
            
            ("How to handle exceptions in Python?",
             "Use try-except blocks: 'try: risky_operation() except Exception as e: handle_error(e)'. You can catch specific exceptions or use finally for cleanup."),
            
            ("How to sort a dictionary by value in Python?",
             "Use sorted(dict.items(), key=lambda x: x[1]) to get sorted items, or dict(sorted(dict.items(), key=lambda x: x[1])) for a new dictionary."),
            
            ("How to use lambda functions in Python?",
             "Lambda functions are anonymous functions: lambda x: x**2. Use them with map, filter, or sorted. Example: list(map(lambda x: x*2, [1,2,3]))."),
            
            # Machine Learning
            ("How to split data for training and testing?",
             "Use sklearn.model_selection.train_test_split(X, y, test_size=0.2, random_state=42) to split data into training and testing sets."),
            
            ("How to normalize data for machine learning?",
             "Use sklearn.preprocessing.StandardScaler() to standardize features by removing mean and scaling to unit variance, or MinMaxScaler() for range [0,1]."),
            
            ("How to handle imbalanced datasets?",
             "Use techniques like SMOTE for oversampling minority class, undersampling majority class, or use class_weight='balanced' in sklearn models."),
            
            ("How to evaluate classification model?",
             "Use metrics like accuracy, precision, recall, F1-score, and confusion matrix. sklearn.metrics provides classification_report() for comprehensive evaluation."),
            
            ("How to prevent overfitting in machine learning?",
             "Use cross-validation, regularization (L1/L2), early stopping, dropout (for neural networks), and ensure sufficient training data."),
            
            # Web Development
            ("How to make HTTP requests in Python?",
             "Use requests library: requests.get(url) for GET requests, requests.post(url, json=data) for POST. Always handle response.status_code."),
            
            ("How to parse JSON in Python?",
             "Use json.loads(json_string) to parse JSON string to Python dict, or json.dumps(python_dict) to convert dict to JSON string."),
            
            ("How to create REST API in Python?",
             "Use FastAPI or Flask. With FastAPI: app = FastAPI(); @app.get('/api') def endpoint(): return {'data': 'value'}. Then run with uvicorn."),
            
            ("How to handle CORS in FastAPI?",
             "Use from fastapi.middleware.cors import CORSMiddleware, then add app.add_middleware(CORSMiddleware, allow_origins=['*'])."),
            
            # Database
            ("How to connect to database in Python?",
             "Use appropriate driver: sqlite3 for SQLite, psycopg2 for PostgreSQL, pymysql for MySQL. Use context managers for safe connections."),
            
            ("How to prevent SQL injection?",
             "Use parameterized queries: cursor.execute('SELECT * FROM table WHERE id = ?', (user_id,)). Never use string concatenation for queries."),
            
            # More questions...
            ("How to create virtual environment in Python?",
             "Use 'python -m venv venv_name' to create, then activate with 'source venv_name/bin/activate' (Linux/Mac) or 'venv_name\\Scripts\\activate' (Windows)."),
            
            ("How to install packages in Python?",
             "Use pip: 'pip install package_name'. For specific versions: 'pip install package_name==1.2.3'. Use requirements.txt: 'pip install -r requirements.txt'."),
            
            ("How to time code execution in Python?",
             "Use time.time(): start = time.time(); code_to_time(); elapsed = time.time() - start. Or use timeit module for precise benchmarking."),
            
            ("How to work with dates in Python?",
             "Use datetime module: from datetime import datetime; now = datetime.now(); formatted = now.strftime('%Y-%m-%d %H:%M:%S')."),
        ]
        
        # Expand dataset by creating variations
        expanded_pairs = []
        for q, a in qa_pairs:
            expanded_pairs.append((q, a))
            # Add variations
            if "pandas" in q.lower():
                expanded_pairs.append((q.replace("pandas", "pd"), a))
            if "DataFrame" in q:
                expanded_pairs.append((q.replace("DataFrame", "dataframe"), a))
        
        # Duplicate to reach desired size
        while len(expanded_pairs) < num_samples:
            expanded_pairs.extend(qa_pairs[:min(len(qa_pairs), num_samples - len(expanded_pairs))])
        
        # Create DataFrame
        df = pd.DataFrame(expanded_pairs[:num_samples], columns=['question', 'answer'])
        df['id'] = range(len(df))
        
        return df

# =================================================================
# SECTION 5: EMBEDDING SERVICE
# =================================================================

class EmbeddingService:
    """Free local embedding service using Sentence Transformers"""
    
    def __init__(self, model_name: str = "all-MiniLM-L6-v2"):
        """Initialize embedding model"""
        print(f"ðŸ”„ Loading embedding model: {model_name}...")
        self.model = SentenceTransformer(model_name)
        self.dimension = self.model.get_sentence_embedding_dimension()
        self.cache = {}
        print(f"âœ… Model loaded! Embedding dimension: {self.dimension}")
    
    def get_embedding(self, text: str, use_cache: bool = True) -> np.ndarray:
        """Get embedding for single text"""
        if use_cache and text in self.cache:
            return self.cache[text]
        
        embedding = self.model.encode(text, show_progress_bar=False)
        
        if use_cache:
            self.cache[text] = embedding
        
        return embedding
    
    def get_embeddings(self, texts: List[str], batch_size: int = 32) -> np.ndarray:
        """Get embeddings for multiple texts"""
        print(f"  Generating embeddings for {len(texts)} texts...")
        embeddings = self.model.encode(
            texts, 
            batch_size=batch_size,
            show_progress_bar=True,
            convert_to_numpy=True
        )
        return embeddings
    
    def clear_cache(self):
        """Clear embedding cache"""
        self.cache.clear()

# =================================================================
# SECTION 6: SEARCH SERVICE (FAISS)
# =================================================================

class SearchService:
    """Free similarity search using FAISS"""
    
    def __init__(self, dimension: int):
        """Initialize FAISS index"""
        self.dimension = dimension
        self.index = None
        self.documents = []
        self.embeddings = None
        
    def build_index(self, embeddings: np.ndarray, documents: List[Dict],
                   use_approximate: bool = True):
        """Build FAISS index"""
        print(f"\nðŸ”„ Building FAISS index...")
        print(f"  Documents: {len(documents)}")
        print(f"  Dimension: {self.dimension}")
        print(f"  Approximate: {use_approximate}")
        
        self.documents = documents
        self.embeddings = embeddings.astype('float32')
        
        # Normalize embeddings for cosine similarity
        faiss.normalize_L2(self.embeddings)
        
        if use_approximate and len(documents) > 100:
            # Use IVF index for approximate search
            nlist = min(int(np.sqrt(len(documents))), 100)
            quantizer = faiss.IndexFlatIP(self.dimension)
            self.index = faiss.IndexIVFFlat(quantizer, self.dimension, nlist)
            self.index.train(self.embeddings)
            print(f"  Using IVF index with {nlist} clusters")
        else:
            # Use flat index for exact search
            self.index = faiss.IndexFlatIP(self.dimension)
            print(f"  Using flat index (exact search)")
        
        self.index.add(self.embeddings)
        print(f"âœ… Index built with {self.index.ntotal} vectors")
    
    def search(self, query_embedding: np.ndarray, top_k: int = 3,
              use_approximate: bool = True) -> List[Tuple[Dict, float]]:
        """Search for similar documents"""
        
        if self.index is None:
            raise ValueError("Index not built. Call build_index first.")
        
        # Normalize query
        query_embedding = query_embedding.astype('float32').reshape(1, -1)
        faiss.normalize_L2(query_embedding)
        
        # Set search parameters for IVF index
        if isinstance(self.index, faiss.IndexIVFFlat):
            nprobe = 10 if use_approximate else self.index.nlist
            self.index.nprobe = nprobe
        
        # Search
        similarities, indices = self.index.search(query_embedding, top_k)
        
        # Format results
        results = []
        for idx, score in zip(indices[0], similarities[0]):
            if idx < len(self.documents):
                doc = self.documents[idx].copy()
                doc['similarity_score'] = float(score)
                results.append((doc, float(score)))
        
        return results
    
    def get_document_by_id(self, doc_id: int) -> Optional[Dict]:
        """Get document by ID"""
        for doc in self.documents:
            if doc['id'] == doc_id:
                return doc
        return None

# =================================================================
# SECTION 7: LLM SERVICE (FREE GROQ API)
# =================================================================

class LLMService:
    """Free LLM service using Groq API"""
    
    def __init__(self, config: SystemConfig):
        self.config = config
        
    def generate_answer(self, query: str, context_docs: List[Dict]) -> str:
        """Generate answer using retrieved context"""
        
        if not self.config.groq_api_key:
            # Fallback to simple context-based answer
            return self._fallback_answer(query, context_docs)
        
        # Build context from retrieved documents
        context = self._build_context(context_docs)
        
        # Create prompt
        prompt = self._create_prompt(query, context)
        
        # Call Groq API
        try:
            response = self._call_groq_api(prompt)
            return response
        except Exception as e:
            print(f"âš ï¸  LLM API error: {e}")
            return self._fallback_answer(query, context_docs)
    
    def _build_context(self, docs: List[Dict]) -> str:
        """Build context from documents"""
        context_parts = []
        for i, doc in enumerate(docs, 1):
            context_parts.append(
                f"Document {i}:\n"
                f"Q: {doc['question']}\n"
                f"A: {doc['answer']}\n"
            )
        return "\n".join(context_parts)
    
    def _create_prompt(self, query: str, context: str) -> str:
        """Create prompt for LLM"""
        return f"""You are a helpful programming assistant. Use the following context to answer the question.

Context:
{context}

Question: {query}

Instructions:
1. Answer based on the provided context
2. Be concise and practical
3. Include code examples if relevant
4. If context doesn't fully answer, provide general guidance

Answer:"""
    
    def _call_groq_api(self, prompt: str) -> str:
        """Call Groq API"""
        headers = {
            "Authorization": f"Bearer {self.config.groq_api_key}",
            "Content-Type": "application/json"
        }
        
        payload = {
            "model": self.config.groq_model,
            "messages": [
                {"role": "system", "content": "You are a helpful programming assistant."},
                {"role": "user", "content": prompt}
            ],
            "temperature": self.config.temperature,
            "max_tokens": self.config.max_tokens
        }
        
        response = requests.post(
            self.config.groq_api_url,
            headers=headers,
            json=payload,
            timeout=30
        )
        response.raise_for_status()
        
        result = response.json()
        return result['choices'][0]['message']['content'].strip()
    
    def _fallback_answer(self, query: str, docs: List[Dict]) -> str:
        """Fallback answer without LLM"""
        if not docs:
            return "I couldn't find relevant information to answer your question."
        
        # Use the top document's answer
        top_doc = docs[0]
        return f"{top_doc['answer']}\n\n(Based on: {top_doc['question']})"

# =================================================================
# SECTION 8: Q&A SERVICE
# =================================================================

class QAService:
    """Main Q&A orchestration service"""
    
    def __init__(self, config: SystemConfig):
        self.config = config
        self.embedding_service = EmbeddingService(config.embedding_model_name)
        self.search_service = SearchService(self.embedding_service.dimension)
        self.llm_service = LLMService(config)
        self.is_initialized = False
        
    def initialize(self, documents_df: pd.DataFrame):
        """Initialize the system with documents"""
        print("\n" + "="*60)
        print("ðŸš€ INITIALIZING Q&A SYSTEM")
        print("="*60)
        
        # Generate embeddings
        print("\nðŸ“Š Processing documents...")
        texts = documents_df['question'].tolist()
        embeddings = self.embedding_service.get_embeddings(texts)
        
        # Prepare documents
        documents = []
        for idx, row in documents_df.iterrows():
            documents.append({
                'id': int(row['id']),
                'question': row['question'],
                'answer': row['answer']
            })
        
        # Build search index
        self.search_service.build_index(
            embeddings, 
            documents,
            use_approximate=self.config.use_approximate_search
        )
        
        self.is_initialized = True
        print("\nâœ… System initialized successfully!")
        print("="*60)
    
    def answer_question(self, query: str, top_k: int = None,
                       use_approximate: bool = None) -> Dict:
        """Answer a question"""
        start_time = time.time()
        
        if not self.is_initialized:
            raise ValueError("System not initialized. Call initialize() first.")
        
        # Set defaults
        if top_k is None:
            top_k = self.config.top_k_results
        if use_approximate is None:
            use_approximate = self.config.use_approximate_search
        
        # Get query embedding
        query_embedding = self.embedding_service.get_embedding(query)
        
        # Search for similar documents
        results = self.search_service.search(
            query_embedding,
            top_k=top_k,
            use_approximate=use_approximate
        )
        
        if not results:
            return {
                'success': False,
                'query': query,
                'answer': "No relevant documents found.",
                'source_document': None,
                'search_method': 'approximate' if use_approximate else 'exact',
                'latency_ms': (time.time() - start_time) * 1000
            }
        
        # Generate answer using LLM
        context_docs = [doc for doc, score in results]
        answer = self.llm_service.generate_answer(query, context_docs)
        
        # Format response
        latency_ms = (time.time() - start_time) * 1000
        
        return {
            'success': True,
            'query': query,
            'answer': answer,
            'source_document': context_docs[0],
            'all_sources': context_docs,
            'search_method': 'approximate' if use_approximate else 'exact',
            'latency_ms': latency_ms
        }
    
    def get_health_status(self) -> Dict:
        """Get system health status"""
        return {
            'status': 'healthy' if self.is_initialized else 'not_initialized',
            'database_info': {
                'total_documents': len(self.search_service.documents),
                'embedding_dimension': self.embedding_service.dimension,
                'index_size': self.search_service.index.ntotal if self.search_service.index else 0
            },
            'models': {
                'embedding_model': self.config.embedding_model_name,
                'llm_model': self.config.groq_model,
                'api_configured': bool(self.config.groq_api_key)
            }
        }

# =================================================================
# SECTION 9: FASTAPI APPLICATION
# =================================================================

# Global QA service instance
qa_service = None
app = FastAPI(
    title="Semantic Search & Q&A System",
    description="Production-ready semantic search using free tools",
    version="1.0.0"
)

@app.get("/")
async def root():
    """Root endpoint"""
    return {
        "message": "Semantic Search & Q&A System",
        "version": "1.0.0",
        "endpoints": {
            "health": "/health",
            "search": "/search",
            "docs": "/docs"
        }
    }

@app.get("/health")
async def health_check():
    """Health check endpoint"""
    if qa_service is None:
        raise HTTPException(status_code=503, detail="Service not initialized")
    
    return qa_service.get_health_status()

@app.post("/search", response_model=SearchResponse)
async def search_post(request: SearchRequest):
    """Search endpoint (POST)"""
    if qa_service is None:
        raise HTTPException(status_code=503, detail="Service not initialized")
    
    try:
        result = qa_service.answer_question(
            query=request.query,
            top_k=request.top_k,
            use_approximate=request.use_approximate
        )
        return SearchResponse(**result)
    except Exception as e:
        raise HTTPException(status_code=500, detail=str(e))

@app.get("/search")
async def search_get(
    query: str = Query(..., description="Search query"),
    use_approximate: bool = Query(True, description="Use approximate search"),
    top_k: int = Query(1, description="Number of results")
):
    """Search endpoint (GET)"""
    if qa_service is None:
        raise HTTPException(status_code=503, detail="Service not initialized")
    
    try:
        result = qa_service.answer_question(
            query=query,
            top_k=top_k,
            use_approximate=use_approximate
        )
        return result
    except Exception as e:
        raise HTTPException(status_code=500, detail=str(e))

@app.get("/documents/{doc_id}")
async def get_document(doc_id: int):
    """Get document by ID"""
    if qa_service is None:
        raise HTTPException(status_code=503, detail="Service not initialized")
    
    doc = qa_service.search_service.get_document_by_id(doc_id)
    if doc is None:
        raise HTTPException(status_code=404, detail="Document not found")
    
    return doc

# =================================================================
# SECTION 10: PYTHON CLIENT
# =================================================================

class QAClient:
    """Python client for Q&A system"""
    
    def __init__(self, base_url: str = "http://localhost:8000"):
        self.base_url = base_url
    
    def health_check(self) -> Dict:
        """Check system health"""
        response = requests.get(f"{self.base_url}/health")
        response.raise_for_status()
        return response.json()
    
    def ask_question(self, question: str, use_approximate: bool = True,
                    top_k: int = 1) -> Dict:
        """Ask a question"""
        payload = {
            "query": question,
            "use_approximate": use_approximate,
            "top_k": top_k
        }
        
        response = requests.post(f"{self.base_url}/search", json=payload)
        response.raise_for_status()
        return response.json()
    
    def get_document(self, doc_id: int) -> Dict:
        """Get document by ID"""
        response = requests.get(f"{self.base_url}/documents/{doc_id}")
        response.raise_for_status()
        return response.json()

# =================================================================
# SECTION 11: INTERACTIVE DEMO
# =================================================================

class InteractiveDemo:
    """Interactive demo interface"""
    
    def __init__(self, qa_service: QAService):
        self.qa_service = qa_service
    
    def run_demo_queries(self):
        """Run demo queries"""
        demo_queries = [
            "How do I concatenate dataframes in pandas?",
            "How to handle missing data?",
            "How to create a REST API in Python?",
            "How to prevent SQL injection?",
            "How to normalize data for machine learning?"
        ]
        
        print("\n" + "="*60)
        print("ðŸŽ¯ RUNNING DEMO QUERIES")
        print("="*60)
        
        for i, query in enumerate(demo_queries, 1):
            print(f"\n{'â”€'*60}")
            print(f"Query {i}: {query}")
            print(f"{'â”€'*60}")
            
            result = self.qa_service.answer_question(query, top_k=1)
            
            print(f"\nðŸ“Š Answer:")
            print(f"{result['answer']}")
            print(f"\nðŸ“„ Source: {result['source_document']['question']}")
            print(f"ðŸŽ¯ Similarity: {result['source_document']['similarity_score']:.3f}")
            print(f"âš¡ Latency: {result['latency_ms']:.1f}ms")
            
            time.sleep(1)
        
        print("\n" + "="*60)
        print("âœ… Demo completed!")
        print("="*60)
    
    def interactive_mode(self):
        """Interactive Q&A mode"""
        print("\n" + "="*60)
        print("ðŸ’¬ INTERACTIVE Q&A MODE")
        print("="*60)
        print("Ask questions (type 'quit' to exit, 'demo' for demo queries)")
        print("="*60 + "\n")
        
        while True:
            try:
                query = input("â“ Your question: ").strip()
                
                if not query:
                    continue
                
                if query.lower() in ['quit', 'exit', 'q']:
                    print("\nðŸ‘‹ Goodbye!")
                    break
                
                if query.lower() == 'demo':
                    self.run_demo_queries()
                    continue
                
                # Process query
                print("\nðŸ” Searching...")
                result = self.qa_service.answer_question(query, top_k=1)
                
                print(f"\nðŸ’¡ Answer:")
                print(f"{result['answer']}")
                print(f"\nðŸ“Š Stats:")
                print(f"  Similarity: {result['source_document']['similarity_score']:.3f}")
                print(f"  Latency: {result['latency_ms']:.1f}ms")
                print(f"  Method: {result['search_method']}")
                print()
                
            except KeyboardInterrupt:
                print("\n\nðŸ‘‹ Goodbye!")
                break
            except Exception as e:
                print(f"\nâŒ Error: {e}\n")

# =================================================================
# SECTION 12: MAIN SETUP & EXECUTION
# =================================================================

def setup_system(api_key: str = "", num_documents: int = 100) -> QAService:
    """Setup the complete system"""
    
    print("\n" + "="*60)
    print("ðŸŽ® SEMANTIC SEARCH & Q&A SYSTEM SETUP")
    print("="*60)
    
    # Create configuration
    config = SystemConfig(
        groq_api_key=api_key,
        embedding_model_name="all-MiniLM-L6-v2",
        top_k_results=3,
        use_approximate_search=True
    )
    
    print(f"\nâš™ï¸  Configuration:")
    print(f"  Embedding Model: {config.embedding_model_name}")
    print(f"  LLM Model: {config.groq_model}")
    print(f"  API Configured: {bool(api_key)}")
    print(f"  Documents: {num_documents}")
    
    # Generate sample data
    print(f"\nðŸ“š Generating sample data...")
    data_generator = DataGenerator()
    documents_df = data_generator.generate_sample_data(num_documents)
    print(f"  âœ“ Generated {len(documents_df)} Q&A pairs")
    
    # Initialize Q&A service
    qa_service = QAService(config)
    qa_service.initialize(documents_df)
    
    return qa_service

def run_server(qa_service_instance: QAService, host: str = "127.0.0.1", 
               port: int = 8000):
    """Run FastAPI server"""
    global qa_service
    qa_service = qa_service_instance
    
    print(f"\nðŸš€ Starting API server on http://{host}:{port}")
    print(f"ðŸ“– API docs available at http://{host}:{port}/docs")
    print(f"âš¡ Health check: http://{host}:{port}/health")
    print("\nPress Ctrl+C to stop the server\n")
    
    uvicorn.run(app, host=host, port=port, log_level="info")

def print_usage_guide():
    """Print usage guide"""
    guide = """
â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—
â•‘          SEMANTIC SEARCH & Q&A SYSTEM - USAGE GUIDE           â•‘
â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

ðŸ†“ FREE TOOLS & APIs:

1. LOCAL EMBEDDINGS (Sentence Transformers):
   â€¢ Model: all-MiniLM-L6-v2
   â€¢ Completely free, runs locally
   â€¢ Fast and efficient
   â€¢ No API key needed

2. FAISS (Facebook AI Similarity Search):
   â€¢ Free, open-source
   â€¢ Lightning-fast similarity search
   â€¢ Supports approximate and exact search

3. GROQ API (Optional, for better answers):
   â€¢ Free API with generous limits
   â€¢ Get key: https://console.groq.com
   â€¢ Models: llama-3.1-8b-instant, llama-3.1-70b-versatile

ðŸ“ QUICK START:

Option 1: Simple Python Usage
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
>>> service = setup_system(api_key="your-key", num_documents=100)
>>> result = service.answer_question("How to merge pandas dataframes?")
>>> print(result['answer'])

Option 2: Interactive Demo
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
>>> service = setup_system(api_key="your-key")
>>> demo = InteractiveDemo(service)
>>> demo.interactive_mode()

Option 3: Run as API Server
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
>>> service = setup_system(api_key="your-key")
>>> run_server(service, host="127.0.0.1", port=8000)
# Then use client or curl to query

Option 4: Python Client
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
>>> client = QAClient("http://localhost:8000")
>>> result = client.ask_question("How to handle missing data?")
>>> print(result['answer'])

ðŸŽ¯ FEATURES:

â€¢ Semantic Search: Find relevant answers even with different wording
â€¢ Fast Response: ~100-300ms per query
â€¢ Approximate Search: Use FAISS for speed on large datasets
â€¢ Exact Search: Use for smaller datasets or highest accuracy
â€¢ REST API: FastAPI with automatic documentation
â€¢ No Dependencies: Runs without external paid services

ðŸ’¡ EXAMPLES:

1. Basic Question:
   Q: "How do I read a CSV file?"
   A: Returns relevant pandas/Python file reading information

2. Semantic Understanding:
   Q: "What's the best way to combine tables?"
   A: Understands you mean DataFrame merging/concatenation

3. Code Examples:
   Q: "Show me how to group data"
   A: Returns pandas groupby examples with explanations

ðŸ“Š CUSTOMIZATION:

â€¢ Adjust top_k for more/fewer results
â€¢ Use approximate=False for exact search
â€¢ Change embedding model for different trade-offs
â€¢ Modify temperature for answer creativity
â€¢ Add your own Q&A data easily

ðŸ”§ TROUBLESHOOTING:

No API Key:
  â†’ System works without Groq API key
  â†’ Answers come directly from database
  â†’ Add API key for enhanced answers

Slow Performance:
  â†’ Use approximate search (default)
  â†’ Reduce top_k parameter
  â†’ Use smaller embedding model

Memory Issues:
  â†’ Reduce num_documents parameter
  â†’ Use CPU-efficient FAISS index
  â†’ Clear embedding cache periodically
"""
    print(guide)

# =================================================================
# SECTION 13: EXAMPLE USAGE SCRIPTS
# =================================================================

def example_1_basic_usage():
    """Example 1: Basic usage without API"""
    print("\n" + "="*60)
    print("ðŸ“ EXAMPLE 1: BASIC USAGE (No API Key Required)")
    print("="*60)
    
    # Setup system without API key (uses fallback answers)
    service = setup_system(api_key="", num_documents=50)
    
    # Ask questions
    questions = [
        "How to concatenate dataframes?",
        "How to handle missing values?",
        "How to create a REST API?"
    ]
    
    for q in questions:
        print(f"\nâ“ {q}")
        result = service.answer_question(q)
        print(f"ðŸ’¡ {result['answer'][:200]}...")
        print(f"âš¡ {result['latency_ms']:.1f}ms")

def example_2_with_api():
    """Example 2: Usage with Groq API"""
    print("\n" + "="*60)
    print("ðŸ“ EXAMPLE 2: WITH GROQ API (Enhanced Answers)")
    print("="*60)
    
    api_key = input("\nEnter Groq API key (or press Enter to skip): ").strip()
    
    if not api_key:
        print("âš ï¸  No API key provided, using fallback mode")
    
    service = setup_system(api_key=api_key, num_documents=100)
    
    # Interactive demo
    demo = InteractiveDemo(service)
    demo.run_demo_queries()

def example_3_api_server():
    """Example 3: Run as API server"""
    print("\n" + "="*60)
    print("ðŸ“ EXAMPLE 3: API SERVER MODE")
    print("="*60)
    
    api_key = input("\nEnter Groq API key (optional): ").strip()
    
    service = setup_system(api_key=api_key, num_documents=100)
    
    print("\nðŸŽ¯ Starting server...")
    print("   You can now use curl or HTTP clients to query the API")
    print("\n   Examples:")
    print('   curl "http://localhost:8000/health"')
    print('   curl -X POST "http://localhost:8000/search" \\')
    print('        -H "Content-Type: application/json" \\')
    print('        -d \'{"query": "How to merge dataframes?"}\'')
    
    run_server(service)

def example_4_client_usage():
    """Example 4: Using Python client"""
    print("\n" + "="*60)
    print("ðŸ“ EXAMPLE 4: PYTHON CLIENT USAGE")
    print("="*60)
    
    # Note: Server must be running first!
    print("\nâš ï¸  Make sure API server is running first!")
    print("    Run example_3_api_server() in another terminal\n")
    
    proceed = input("Is the server running? (y/n): ").strip().lower()
    
    if proceed != 'y':
        print("Start the server first with example_3_api_server()")
        return
    
    # Create client
    client = QAClient("http://localhost:8000")
    
    try:
        # Check health
        print("\nðŸ” Checking health...")
        health = client.health_check()
        print(f"âœ… Status: {health['status']}")
        print(f"ðŸ“Š Documents: {health['database_info']['total_documents']}")
        
        # Ask questions
        questions = [
            "How to read a file in Python?",
            "How to prevent SQL injection?",
            "How to normalize data for ML?"
        ]
        
        for q in questions:
            print(f"\nâ“ {q}")
            result = client.ask_question(q)
            print(f"ðŸ’¡ {result['answer'][:150]}...")
            print(f"ðŸŽ¯ Similarity: {result['source_document']['similarity_score']:.3f}")
            
    except requests.exceptions.ConnectionError:
        print("\nâŒ Could not connect to server!")
        print("   Make sure the API server is running")

def example_5_custom_data():
    """Example 5: Adding custom data"""
    print("\n" + "="*60)
    print("ðŸ“ EXAMPLE 5: CUSTOM DATA")
    print("="*60)
    
    # Create custom Q&A pairs
    custom_qa = pd.DataFrame([
        {
            'id': 0,
            'question': 'How does your company handle data privacy?',
            'answer': 'We use end-to-end encryption and comply with GDPR and CCPA regulations.'
        },
        {
            'id': 1,
            'question': 'What are your support hours?',
            'answer': 'Our support team is available 24/7 via email, chat, and phone.'
        },
        {
            'id': 2,
            'question': 'How do I reset my password?',
            'answer': 'Click "Forgot Password" on the login page and follow the email instructions.'
        },
        {
            'id': 3,
            'question': 'What payment methods do you accept?',
            'answer': 'We accept credit cards, PayPal, bank transfers, and cryptocurrency.'
        },
        {
            'id': 4,
            'question': 'How do I cancel my subscription?',
            'answer': 'Go to Account Settings > Subscription > Cancel. No cancellation fees apply.'
        }
    ])
    
    print(f"\nðŸ“š Created {len(custom_qa)} custom Q&A pairs")
    
    # Initialize system
    config = SystemConfig(groq_api_key="")
    service = QAService(config)
    service.initialize(custom_qa)
    
    # Test queries
    test_queries = [
        "How do you protect my data?",
        "When can I get help?",
        "I forgot my password"
    ]
    
    for q in test_queries:
        print(f"\nâ“ {q}")
        result = service.answer_question(q)
        print(f"ðŸ’¡ {result['answer']}")
        print(f"ðŸŽ¯ Matched: {result['source_document']['question']}")

def example_6_benchmark():
    """Example 6: Performance benchmarking"""
    print("\n" + "="*60)
    print("ðŸ“ EXAMPLE 6: PERFORMANCE BENCHMARK")
    print("="*60)
    
    service = setup_system(api_key="", num_documents=100)
    
    test_queries = [
        "How to merge dataframes?",
        "How to handle exceptions?",
        "How to create API?",
        "How to read files?",
        "How to sort data?"
    ]
    
    print("\nðŸƒ Running benchmark...")
    print(f"Queries: {len(test_queries)}")
    
    # Approximate search
    print("\nðŸ“Š APPROXIMATE SEARCH:")
    approx_times = []
    for q in test_queries:
        start = time.time()
        result = service.answer_question(q, use_approximate=True)
        elapsed = (time.time() - start) * 1000
        approx_times.append(elapsed)
        print(f"  {q[:40]:40s} {elapsed:6.1f}ms")
    
    # Exact search
    print("\nðŸ“Š EXACT SEARCH:")
    exact_times = []
    for q in test_queries:
        start = time.time()
        result = service.answer_question(q, use_approximate=False)
        elapsed = (time.time() - start) * 1000
        exact_times.append(elapsed)
        print(f"  {q[:40]:40s} {elapsed:6.1f}ms")
    
    # Summary
    print("\nðŸ“ˆ SUMMARY:")
    print(f"  Approximate Avg: {np.mean(approx_times):.1f}ms")
    print(f"  Exact Avg: {np.mean(exact_times):.1f}ms")
    print(f"  Speedup: {np.mean(exact_times)/np.mean(approx_times):.2f}x")

# =================================================================
# SECTION 14: MAIN MENU
# =================================================================

def main_menu():
    """Interactive main menu"""
    print("\n" + "="*60)
    print("ðŸŽ® SEMANTIC SEARCH & Q&A SYSTEM")
    print("="*60)
    print("\nChoose an option:")
    print("\n  1. ðŸ“– Show Usage Guide")
    print("  2. ðŸš€ Quick Start (Basic Usage)")
    print("  3. ðŸ¤– Quick Start (With Groq API)")
    print("  4. ðŸŒ Run API Server")
    print("  5. ðŸ’¬ Interactive Q&A Mode")
    print("  6. ðŸ“Š Run Demo Queries")
    print("  7. ðŸ”§ Custom Data Example")
    print("  8. âš¡ Performance Benchmark")
    print("  9. ðŸ“ Python Client Example")
    print("  0. âŒ Exit")
    print("\n" + "="*60)
    
    choice = input("\nEnter your choice (0-9): ").strip()
    
    if choice == '1':
        print_usage_guide()
    elif choice == '2':
        example_1_basic_usage()
    elif choice == '3':
        example_2_with_api()
    elif choice == '4':
        example_3_api_server()
    elif choice == '5':
        api_key = input("\nEnter Groq API key (optional): ").strip()
        service = setup_system(api_key=api_key, num_documents=100)
        demo = InteractiveDemo(service)
        demo.interactive_mode()
    elif choice == '6':
        api_key = input("\nEnter Groq API key (optional): ").strip()
        service = setup_system(api_key=api_key, num_documents=100)
        demo = InteractiveDemo(service)
        demo.run_demo_queries()
    elif choice == '7':
        example_5_custom_data()
    elif choice == '8':
        example_6_benchmark()
    elif choice == '9':
        example_4_client_usage()
    elif choice == '0':
        print("\nðŸ‘‹ Goodbye!")
        return
    else:
        print("\nâŒ Invalid choice!")
    
    input("\n\nPress Enter to return to menu...")
    main_menu()

# =================================================================
# SECTION 15: AUTO-RUN & QUICK START
# =================================================================

def quick_start():
    """One-command quick start"""
    print("\n" + "="*60)
    print("âš¡ QUICK START - SEMANTIC SEARCH & Q&A SYSTEM")
    print("="*60)
    print("\n100% Free Tools:")
    print("  âœ“ Sentence Transformers (local embeddings)")
    print("  âœ“ FAISS (similarity search)")
    print("  âœ“ Optional: Groq API (free LLM)")
    print("\n" + "="*60)
    
    # Ask for API key
    print("\nðŸ”‘ GROQ API KEY (Optional)")
    print("   Get free key: https://console.groq.com")
    api_key = input("   Enter key (or press Enter to skip): ").strip()
    
    if not api_key:
        print("\nâš ï¸  No API key - using database answers (still works!)")
    
    # Ask for number of documents
    print("\nðŸ“š NUMBER OF DOCUMENTS")
    num_docs = input("   Enter number (default 100): ").strip()
    num_docs = int(num_docs) if num_docs.isdigit() else 100
    
    # Setup system
    service = setup_system(api_key=api_key, num_documents=num_docs)
    
    # Choose mode
    print("\nðŸŽ¯ CHOOSE MODE:")
    print("  1. Interactive Q&A (recommended)")
    print("  2. Run Demo Queries")
    print("  3. Start API Server")
    
    mode = input("\nEnter choice (1-3): ").strip()
    
    if mode == '1':
        demo = InteractiveDemo(service)
        demo.interactive_mode()
    elif mode == '2':
        demo = InteractiveDemo(service)
        demo.run_demo_queries()
    elif mode == '3':
        run_server(service)
    else:
        print("\nDefaulting to interactive mode...")
        demo = InteractiveDemo(service)
        demo.interactive_mode()

# =================================================================
# NOTEBOOK EXECUTION
# =================================================================

if __name__ == "__main__":
    print("\n" + "="*60)
    print("ðŸŽ® SEMANTIC SEARCH & Q&A SYSTEM - FREE VERSION")
    print("="*60)
    print("\nComplete semantic search system using 100% free tools!")
    print("\nðŸ“ To start:")
    print("   1. Get free Groq API key: https://console.groq.com (optional)")
    print("   2. Run: quick_start()")
    print("   3. Or run: main_menu() for all options")
    print("\nðŸš€ Quick commands:")
    print("   â€¢ quick_start()         - One-command start")
    print("   â€¢ main_menu()           - Interactive menu")
    print("   â€¢ print_usage_guide()   - Complete guide")
    print("\n" + "="*60)

# Uncomment to auto-run:
# quick_start()

"""
=================================================================
USAGE EXAMPLES - COPY & PASTE THESE INTO YOUR NOTEBOOK
=================================================================

# EXAMPLE 1: Simplest possible usage
>>> service = setup_system(num_documents=50)
>>> result = service.answer_question("How to merge dataframes?")
>>> print(result['answer'])

# EXAMPLE 2: With Groq API for better answers
>>> service = setup_system(api_key="your-groq-key", num_documents=100)
>>> result = service.answer_question("How to handle missing data?")
>>> print(result['answer'])

# EXAMPLE 3: Interactive mode
>>> service = setup_system()
>>> demo = InteractiveDemo(service)
>>> demo.interactive_mode()  # Type questions interactively

# EXAMPLE 4: Run as API server (in one cell)
>>> service = setup_system(api_key="your-key")
>>> run_server(service)  # Access at http://localhost:8000

# EXAMPLE 5: Use Python client (in another notebook/script)
>>> client = QAClient("http://localhost:8000")
>>> result = client.ask_question("How to create REST API?")
>>> print(result)

# EXAMPLE 6: Custom company FAQ data
>>> custom_data = pd.DataFrame([
...     {'id': 0, 'question': 'What are your hours?', 
...      'answer': 'We are open 24/7'},
...     {'id': 1, 'question': 'How to contact support?',
...      'answer': 'Email us at support@company.com'}
... ])
>>> service = QAService(SystemConfig())
>>> service.initialize(custom_data)
>>> result = service.answer_question("When are you open?")

# EXAMPLE 7: Batch questions
>>> questions = [
...     "How to read CSV?",
...     "How to handle exceptions?",
...     "How to create virtual environment?"
... ]
>>> service = setup_system()
>>> for q in questions:
...     result = service.answer_question(q)
...     print(f"Q: {q}")
...     print(f"A: {result['answer']}\n")

# EXAMPLE 8: Compare search methods
>>> service = setup_system()
>>> q = "How to merge dataframes?"
>>> approx = service.answer_question(q, use_approximate=True)
>>> exact = service.answer_question(q, use_approximate=False)
>>> print(f"Approx: {approx['latency_ms']:.1f}ms")
>>> print(f"Exact: {exact['latency_ms']:.1f}ms")

=================================================================
CURL EXAMPLES (when API server is running)
=================================================================

# Health check
curl http://localhost:8000/health

# Search with GET
curl "http://localhost:8000/search?query=How%20to%20merge%20dataframes&top_k=1"

# Search with POST
curl -X POST "http://localhost:8000/search" \
  -H "Content-Type: application/json" \
  -d '{"query": "How to handle missing data?", "top_k": 1}'

# Get specific document
curl http://localhost:8000/documents/0

=================================================================
"""
